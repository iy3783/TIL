# openstack 마지막

## nova cli 사용하기



- openstack flavor create --id 1 --vcpus 2 --ram 2048 --disk 8 m1.small 
  - flavor생성 명령어
- openstack server create --flavor m1.small --image CentOS7.3 --security-group default CentOS7.3
- 뉴트론이 깔려있어야 작업가능

## neutron

- openstack networking

  - VNI(virtual networking infra) 가상네트워크 레이어에서 pni에 접속하는 것을 관리
  - firewalls,load balancers,vpn 등의 서비스 포함한 네트워크 토폴리지들을 tenants가 생성하도록한다.
  - networking 다음 객체 추상화제공
    - 네트워크
    - 서브넷
    - 라우터

- 뉴트론은 여러 plug in 제공하고 기본은 kvm qemu 하이퍼바이저가 지원하는 플러그인이 제일 많다.

- VIO : vmware 하이퍼바이저를 가지는 오픈스택 제공

- 뉴트론 분리구조

  - 컨트롤러 노드 

    - 뉴트론 서버
    - 뉴트론 plugin agent

  - 네트워크 노드 

    - 뉴트론 plugin agent
    - 뉴트론 L3 agent(라우팅)

    - 뉴트론 DHCP agent
    - 뉴트론 layer2 agent(OVS)

  - 컴퓨트 노드

    - 뉴트론 plugin agent
    - 뉴트론 layer2 agent(OVS)

- 원래 네트워크 노드의 라우터 통해 외부 인터넷과 연결했었지만 지금은 부하를 줄이기 위해 compute노드를 외부 인터넷과 다이렉트로 연결하는 구조로 만든다.

- 각 노드마다 ML2(플러그인 , osi 2계층) 가 들어가 있다. => 각 노드마다 L2 매커니즘 드라이버를 각각 설정해서 복합적으로 사용가능 => 하나의 계층에서 여러 네트워킹 타입 지원

- 2계층 타입드라이버 종류
  - vlan: 여러개의 가상 lan 만들어서 브로드캐스트 도메인을 여러개로 나눔. 각 도메인들은 라우터통해 통신한다.물리적으로는 하나의 망이다.
  - flat : 하나로 쭉 이어져있는 ip(flat하다) , 하나의 네트워크로 구성 , 여러개의 flat 네트워크 구성시 라우터로 통신한다
  - local : host only bridge : 같은 호스트들만 통신 가능

- vlan들이 있고 이런 vlan 들이 물리적 스위치에도 연결이 된다.

  - 패킷자체가 브로드 캐스팅 되지않는다. 각 어디로 보낼지 맥주소를 알기때문에(물리적 스위치 이용) 전체로 브로드캐스팅 하지 않는다.
  - 각 인스턴스가 늘어날수록 물리적스위치 포트에 있는 테이블이 커지고 overhead 커진다.
    - => 인스턴스 최대 4천개정도 올릴수 있다.

  - 이를 해결하기 위해 vxlan 나왔다. 클라우드 환경에 적당한 vlan   

  - vxlan은 가상스위치가 알아서 나눠줘서 물리적 오버헤드 없다.
  - UDP 기반 multicasting
  - 클라우드 내의 모든 호스트에 대한 P2P Turnnel을 만든다. 모든 호스트들은 tunnel을 통해 하나의 mesh topology 형성(모두 연결되어있다. 컨트롤에서 컨트롤 ,컴퓨트에서 컴퓨트 , 컴퓨트에서 컨트롤 모두 연결) 
  - 

- ML2 를 이용해서 서로다른 메커니즘 드라이버를 사용해도 ml2 플러그인을 라우터 처럼 사용해서 통신이 가능하게 한다.(둘이 직접 통신 할 필요없다.)



- 네트워킹 옵션 
  - provider networks : 내부망에 다올림 (관리자 생성)
  - self-service networks : 외부 망에 다올림 (사용자생성)



- ```shell
  뉴트론 명령어
  openstack-service status neutron => 뉴트론 서비스 상태 보기
  neutron agent-list
  neutron ext-list => 뉴트론 구성요소들 리스트
  ```

- dvr : distributed virtual router : 컴퓨트 노드 다이렉트로 외부 인터넷 연결하는 라우터 => 우리는 이거 대신 리눅스 브릿지 agent이용할것이다.

- ```
  ovs-vsctl show 
  가상스위치 목록 보여준다.
  
  ```



- 네트워크 순서

  ```
  instance
  linux bridge( 여기에 네트워크 방화벽 있다.)
  ovs integration bridge(br-int,내부 아이피, 고정아이피)
  ovs tunnel bridge (유동 IP로 밖과 연결) 
  ############컴퓨트노드#############################
  vxlan/gre Tunnel 
  ############네트워크노드###########################
  ovs tunnel bridge (br-tun)
  ovs integration bridge(br-int)
  ovs externel bridge
  외부인터넷
  ```

- ```shell
  ip netns
  #하면 라우터 정보 나온다.
  ip netns exec qrouter_id정보값 ip addr
  => 해당 라우터와 연결된 정보 알려준다.
  
  
  
  ```

- ```shell
  CLI로 Instance 시작
  ------------------------------
  https://docs.openstack.org/install-guide/launch-instance.html
    217  . keystonerc_demo 
    218  openstack network create selfservice
    219  openstack subnet create --network selfservice   --dns-nameserver 8.8.4.4 --gateway 172.16.1.1   --subnet-range 172.16.1.0/24 selfservice
    220  openstack router create router
    221  openstack router add subnet router selfservice
   
    223  openstack router set router --external-gateway ext1
    224  . keystonerc_admin 
    225  openstack port list --router router
  
    227  openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano
    228  openstack flavor list
    229  . keystonerc_demo 
    230  ls .ssh
    231  openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
    232  openstack keypair list
    233  openstack security group rule create --proto icmp default
    234  openstack security group rule create --proto tcp --dst-port 22 default
    235  openstack image list
    236  ls
    237  openstack image create "cirros-0.3.5" --container-format bare --disk-format qcow2 --file ./cirros-0.3.5-x86_64-disk.img 
    238  openstack image list
    239   openstack network list
  openstack server create --flavor m1.nano --image cirros-0.3.5 \
    --nic net-id=7b6c4fd5-73b9-4f49-8828-bfc3efb20a28 --security-group default \
    --key-name mykey selfservice-instance
  # openstack server list
  
  인스턴스 console 접속
  --------------------------------------------------------------
   1. openstack console url show selfservice-instance (web기반 novnc protocol로 접속 가능)
   2. virsh list --all
      virsh console 1 ( disconnect는 ^] )
   ------------------------------------------------------------- 
    248  openstack floating ip create ext1
    249  openstack server add floating ip selfservice-instance 10.0.0.216
    255  ip netns exec qrouter-b716d035-eaee-4143-8489-c93dfb7241c7 ssh cirros@10.0.0.216
  ```




## cinder

- cinder : 컨트롤러노드 + 컴퓨트노드 + 스토리지 노드(실제 저장공간)

- 구조 

  - cinder API : 인터페이스
  - queue : rabbit mq
  - cinder DB : 디비로 메타데이터 관리
  - scheduler : 
  - cinder volume : 실제 저장공간
  - cinder 백업방법
    - 스냅샷
    - swift 이용한 자체 백업 서비스 : 얘가 더좋음

- LVM 쓴다.(기본적으로): logical volume manager 

  - 일종의 파티션 
  - 물리적으로 하나의 디스크를 가상디스크로 나누어 쓴다.

- ```
  vgs:가상 볼륨 확인 명령어
  pvs:물리 볼륨 확인 명령어
  losetup : 루프백 디바이스 확인
  lsblk : 디스크정보확인
  
  
  
  ```

- iscsi(internet small computer system interface ) : 컴퓨팅 환경에서 데이터 스토리지 시설을 이어주는 IP기반의 스토리지 네트워킹 표준이다. iscsi는 ip망을 통해 scsi 명령을 전달함으로써 인트라넷을 거쳐 데이터 전송을 쉽게 하고 먼 거리에 걸쳐 스토리지를 관리하는 데 쓰인다

  - openstack : cinder
  - aws efs
  - NFS

## swift

- 가공되지 않은 데이터를 http 프로토콜로 전송하는 서비스 

- ```shell
  #swift post c1
  #swift post c2
  #swift upload c1 /etc/passwd
  #swift list c1 –lh
  #cd /var/tmp
  #swift download c1
  ```

- 

## 네트워크 이중화

- 컴퓨트 노드들은 단일 인터페이스 가져서 만약 인터넷 끊기면 컴퓨트 노드 사용 못함
- 이를 방지하기 위해 네트워크인터페이스 2개를 만드는 이중화를 한다.

- 컨트롤러
  - ha 프록시라는 소프트웨어를 사용해서 소프트웨어적 로드밸런싱 해준다.
  - 여러개의 컨트롤러는 mysqlDB는 동기화가 되어야한다.

- 만약 인스턴스들의 스토리지 가지는 compute 노드에서 다운이 된다면 instance들은 모두 사용불가가된다.
  - 인스턴스들의 고 가용성을 위해서는 cinder의 새볼륨을 만들어서 할당해주면서 사용한다.(노다 컴퓨트 노드와 스토리지노드의 분리 필요성)

## 그외공부할거

- HEAT(프로젝트 진행) : aws의 Cloud formation ,같은 템플릿으로 리소스 관리
- ELK : 스택 로그서버사용

